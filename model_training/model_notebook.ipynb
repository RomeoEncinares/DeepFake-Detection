{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cc5c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6ac1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "max_seq_length = 20\n",
    "num_features = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b74d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(img_size, img_size)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27da3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(img_size, img_size, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((img_size, img_size, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44bdc1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_metadata = pd.read_json('dataset/train_sample_videos/metadata.json').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f6a238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir): #df是train_sample_metadata->json的split\n",
    "    num_samples = len(df)\n",
    "    video_paths = list(df.index)\n",
    "    labels = df[\"label\"].values\n",
    "    labels = np.array(labels=='FAKE').astype(np.int)\n",
    "    \n",
    "    frame_masks = np.zeros(shape=(num_samples, max_seq_length), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, max_seq_length, num_features), dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        temp_frame_mask = np.zeros(shape=(1, max_seq_length,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype=\"float32\")\n",
    "        \n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(max_seq_length, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] =feature_extractor.predict(batch[None, j, :])\n",
    "            temp_frame_mask[i, :length] =1 # 1 = not masked, 0 = masked\n",
    "        \n",
    "        frame_features[idx,] =temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] =temp_frame_mask.squeeze()\n",
    "    \n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f8cd809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 3) (40, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df , test_df = train_test_split(train_sample_metadata, test_size=0.1,random_state=42,\n",
    "                                       stratify=train_sample_metadata['label'])\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "383b956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerome\\AppData\\Local\\Temp\\ipykernel_7092\\2542016939.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  labels = np.array(labels=='FAKE').astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (360, 20, 2048)\n",
      "Frame masks in train set: (360, 20)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78345bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAKE', 'REAL']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"label\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69cd0b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.7171\n",
      "Epoch 1: val_loss improved from inf to 0.68722, saving model to models\\\n",
      "8/8 [==============================] - 15s 806ms/step - loss: 0.6911 - accuracy: 0.7171 - val_loss: 0.6872 - val_accuracy: 0.8716\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.7809\n",
      "Epoch 2: val_loss improved from 0.68722 to 0.68140, saving model to models\\\n",
      "8/8 [==============================] - 3s 380ms/step - loss: 0.6868 - accuracy: 0.7809 - val_loss: 0.6814 - val_accuracy: 0.8716\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6824 - accuracy: 0.7809\n",
      "Epoch 3: val_loss improved from 0.68140 to 0.67571, saving model to models\\\n",
      "8/8 [==============================] - 3s 388ms/step - loss: 0.6824 - accuracy: 0.7809 - val_loss: 0.6757 - val_accuracy: 0.8716\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6781 - accuracy: 0.7809\n",
      "Epoch 4: val_loss improved from 0.67571 to 0.67018, saving model to models\\\n",
      "8/8 [==============================] - 3s 365ms/step - loss: 0.6781 - accuracy: 0.7809 - val_loss: 0.6702 - val_accuracy: 0.8716\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6741 - accuracy: 0.7809\n",
      "Epoch 5: val_loss improved from 0.67018 to 0.66464, saving model to models\\\n",
      "8/8 [==============================] - 3s 396ms/step - loss: 0.6741 - accuracy: 0.7809 - val_loss: 0.6646 - val_accuracy: 0.8716\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6700 - accuracy: 0.7809\n",
      "Epoch 6: val_loss improved from 0.66464 to 0.65925, saving model to models\\\n",
      "8/8 [==============================] - 3s 353ms/step - loss: 0.6700 - accuracy: 0.7809 - val_loss: 0.6593 - val_accuracy: 0.8716\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6661 - accuracy: 0.7809\n",
      "Epoch 7: val_loss improved from 0.65925 to 0.65399, saving model to models\\\n",
      "8/8 [==============================] - 3s 335ms/step - loss: 0.6661 - accuracy: 0.7809 - val_loss: 0.6540 - val_accuracy: 0.8716\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6624 - accuracy: 0.7809\n",
      "Epoch 8: val_loss improved from 0.65399 to 0.64876, saving model to models\\\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 0.6624 - accuracy: 0.7809 - val_loss: 0.6488 - val_accuracy: 0.8716\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6586 - accuracy: 0.7809\n",
      "Epoch 9: val_loss improved from 0.64876 to 0.64377, saving model to models\\\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.6586 - accuracy: 0.7809 - val_loss: 0.6438 - val_accuracy: 0.8716\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6548 - accuracy: 0.7809\n",
      "Epoch 10: val_loss improved from 0.64377 to 0.63903, saving model to models\\\n",
      "8/8 [==============================] - 3s 355ms/step - loss: 0.6548 - accuracy: 0.7809 - val_loss: 0.6390 - val_accuracy: 0.8716\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6500 - accuracy: 0.8000\n",
      "Test accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((max_seq_length, num_features))\n",
    "    mask_input = keras.Input((max_seq_length,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"models/\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d80968",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_sequence_model()\n",
    "model.save('models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29437e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
