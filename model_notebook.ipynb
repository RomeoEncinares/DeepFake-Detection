{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c92975e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc5c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50d9c4",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ac1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "max_seq_length = 20\n",
    "num_features = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a2f5b",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bdc1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_metadata = pd.read_json('dataset/train_sample_videos/metadata.json').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8950e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_bound(image, angle):\n",
    "    #rotates an image by the degree angle\n",
    "    # grab the dimensions of the image and then determine the center\n",
    "    (h, w) = image.shape[:2]\n",
    "    (cX, cY) = (w // 2, h // 2)\n",
    "    # grab the rotation matrix (applying the angle to rotate clockwise), then grab the sine and cosine\n",
    "    # (i.e., the rotation components of the matrix)\n",
    "    M = cv2.getRotationMatrix2D((cX, cY), angle, 1.0)\n",
    "    cos = np.abs(M[0, 0])\n",
    "    sin = np.abs(M[0, 1]) \n",
    "    # compute the new bounding dimensions of the image\n",
    "    nW = int((h * sin) + (w * cos))\n",
    "    nH = int((h * cos) + (w * sin)) \n",
    "    # adjust the rotation matrix to take into account translation\n",
    "    M[0, 2] += (nW / 2) - cX\n",
    "    M[1, 2] += (nH / 2) - cY \n",
    "    # perform the actual rotation and return the image\n",
    "    print('rotate')\n",
    "    return cv2.warpAffine(image, M, (nW, nH)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e1bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(img):\n",
    "    detector = MTCNN()   \n",
    "    data=detector.detect_faces(img)\n",
    "    biggest=0\n",
    "    if data !=[]:\n",
    "        for faces in data:\n",
    "            box=faces['box']            \n",
    "            # calculate the area in the image\n",
    "            area = box[3]  * box[2]\n",
    "            if area>biggest:\n",
    "                biggest=area\n",
    "                bbox=box                \n",
    "                keypoints=faces['keypoints']\n",
    "                left_eye=keypoints['left_eye']\n",
    "                right_eye=keypoints['right_eye']                 \n",
    "        lx,ly=left_eye        \n",
    "        rx,ry=right_eye\n",
    "        dx=rx-lx\n",
    "        dy=ry-ly\n",
    "        tan=dy/dx\n",
    "        theta=np.arctan(tan)\n",
    "        theta=np.degrees(theta)    \n",
    "        img=rotate_bound(img, theta)\n",
    "        print('align')\n",
    "        return (True,img)\n",
    "    else:\n",
    "        return (False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b47472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    detector = MTCNN()\n",
    "    data=detector.detect_faces(img)\n",
    "    biggest=0\n",
    "    if data !=[]:\n",
    "        for faces in data:\n",
    "            box=faces['box']            \n",
    "            # calculate the area in the image\n",
    "            area = box[3]  * box[2]\n",
    "            if area>biggest:\n",
    "                biggest=area\n",
    "                bbox=box \n",
    "        bbox[0]= 0 if bbox[0]<0 else bbox[0]\n",
    "        bbox[1]= 0 if bbox[1]<0 else bbox[1]\n",
    "        img=img[bbox[1]: bbox[1]+bbox[3],bbox[0]: bbox[0]+ bbox[2]]\n",
    "        print('crop')\n",
    "        return (True, img) \n",
    "    else:\n",
    "        return (False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd45600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_crop_resize(image): \n",
    "    img=image # read in the image\n",
    "    shape=img.shape\n",
    "    status,img=align(img) # rotates the image for the eyes are horizontal\n",
    "    if status:                \n",
    "        cstatus, img=crop_image(img) # crops the aligned image to return the largest face\n",
    "        if cstatus:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b74d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac1f5b",
   "metadata": {},
   "source": [
    "### Feature Extraction Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27da3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(img_size, img_size, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((img_size, img_size, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ddd8d",
   "metadata": {},
   "source": [
    "### Data Processing Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ca8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path, max_frames=0, resize=(img_size, img_size)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = align_crop_resize(frame)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6a238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = list(df.index)\n",
    "    labels = df[\"label\"].values\n",
    "    labels = np.array(labels=='FAKE').astype(np.int)\n",
    "    \n",
    "    frame_masks = np.zeros(shape=(num_samples, max_seq_length), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, max_seq_length, num_features), dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        temp_frame_mask = np.zeros(shape=(1, max_seq_length,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype=\"float32\")\n",
    "        \n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(max_seq_length, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] =feature_extractor.predict(batch[None, j, :])\n",
    "            temp_frame_mask[i, :length] =1 # 1 = not masked, 0 = masked\n",
    "        \n",
    "        frame_features[idx,] =temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] =temp_frame_mask.squeeze()\n",
    "    \n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8bd88",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f8cd809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 3) (40, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df , test_df = train_test_split(train_sample_metadata, test_size=0.1,random_state=42,\n",
    "                                       stratify=train_sample_metadata['label'])\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "383b956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerome\\AppData\\Local\\Temp\\ipykernel_14052\\3870796331.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  labels = np.array(labels=='FAKE').astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (360, 20, 2048)\n",
      "Frame masks in train set: (360, 20)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78345bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAKE', 'REAL']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"label\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43107cc",
   "metadata": {},
   "source": [
    "### Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69cd0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((max_seq_length, num_features))\n",
    "    mask_input = keras.Input((max_seq_length,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0acc888",
   "metadata": {},
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e309bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.7092\n",
      "Epoch 1: val_loss improved from inf to 0.68736, saving model to models\\\n",
      "8/8 [==============================] - 20s 932ms/step - loss: 0.6913 - accuracy: 0.7092 - val_loss: 0.6874 - val_accuracy: 0.8716\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6871 - accuracy: 0.7809\n",
      "Epoch 2: val_loss improved from 0.68736 to 0.68164, saving model to models\\\n",
      "8/8 [==============================] - 3s 420ms/step - loss: 0.6871 - accuracy: 0.7809 - val_loss: 0.6816 - val_accuracy: 0.8716\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.7809\n",
      "Epoch 3: val_loss improved from 0.68164 to 0.67612, saving model to models\\\n",
      "8/8 [==============================] - 3s 409ms/step - loss: 0.6827 - accuracy: 0.7809 - val_loss: 0.6761 - val_accuracy: 0.8716\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.7809\n",
      "Epoch 4: val_loss improved from 0.67612 to 0.67077, saving model to models\\\n",
      "8/8 [==============================] - 3s 417ms/step - loss: 0.6785 - accuracy: 0.7809 - val_loss: 0.6708 - val_accuracy: 0.8716\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6747 - accuracy: 0.7809\n",
      "Epoch 5: val_loss improved from 0.67077 to 0.66526, saving model to models\\\n",
      "8/8 [==============================] - 3s 421ms/step - loss: 0.6747 - accuracy: 0.7809 - val_loss: 0.6653 - val_accuracy: 0.8716\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6706 - accuracy: 0.7809\n",
      "Epoch 6: val_loss improved from 0.66526 to 0.65992, saving model to models\\\n",
      "8/8 [==============================] - 3s 419ms/step - loss: 0.6706 - accuracy: 0.7809 - val_loss: 0.6599 - val_accuracy: 0.8716\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.7809\n",
      "Epoch 7: val_loss improved from 0.65992 to 0.65465, saving model to models\\\n",
      "8/8 [==============================] - 3s 412ms/step - loss: 0.6668 - accuracy: 0.7809 - val_loss: 0.6547 - val_accuracy: 0.8716\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.7809\n",
      "Epoch 8: val_loss improved from 0.65465 to 0.64972, saving model to models\\\n",
      "8/8 [==============================] - 3s 428ms/step - loss: 0.6627 - accuracy: 0.7809 - val_loss: 0.6497 - val_accuracy: 0.8716\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6591 - accuracy: 0.7809\n",
      "Epoch 9: val_loss improved from 0.64972 to 0.64468, saving model to models\\\n",
      "8/8 [==============================] - 3s 425ms/step - loss: 0.6591 - accuracy: 0.7809 - val_loss: 0.6447 - val_accuracy: 0.8716\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6556 - accuracy: 0.7809\n",
      "Epoch 10: val_loss improved from 0.64468 to 0.63968, saving model to models\\\n",
      "8/8 [==============================] - 3s 415ms/step - loss: 0.6556 - accuracy: 0.7809 - val_loss: 0.6397 - val_accuracy: 0.8716\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6505 - accuracy: 0.8000\n",
      "Test accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"models/\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b27013",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7d80968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/assets\n"
     ]
    }
   ],
   "source": [
    "model = get_sequence_model()\n",
    "model.save('models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598e9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
